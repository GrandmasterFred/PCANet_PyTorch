{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This will train the network I guess\n",
    "And then I will likely need another file to test the network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : search-EXP-20230815-112531\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import torch\n",
    "import utils\n",
    "import logging\n",
    "import argparse\n",
    "from pcanet import PCANet\n",
    "from dataset_mnist import load_train_mnist\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from utils import MyLogger\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "parser = argparse.ArgumentParser(\"PCANet\")\n",
    "# this one fixes the issue in ipython apparently\n",
    "# https://stackoverflow.com/questions/42249982/systemexit-2-error-when-calling-parse-args-within-ipython\n",
    "parser.add_argument('-f')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--dataset_name', type=str, default='mnist', help='mnist or cifar10')\n",
    "parser.add_argument('--dataset_path', type=str, default='/dataset/', help='location of the data corpus')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='batch size')\n",
    "# this train portion can be modified to make it faster i guess, default is 1.0\n",
    "parser.add_argument('--train_portion', type=float, default=0.001, help='portion of training data')\n",
    "parser.add_argument('--stages', type=int, default=2, help='the number of stages')\n",
    "parser.add_argument('--filter_shape', type=list, default=[7, 7], help='patch size')\n",
    "parser.add_argument('--stages_channels', type=list, default=[8, 8], help='channels in different stages')\n",
    "parser.add_argument('--block_size', type=int, default=7, help='the size of blocks')\n",
    "parser.add_argument('--block_overlap', type=float, default=0.5, help='the rate of overlap between blocks')\n",
    "parser.add_argument('--save', type=str, default='EXP', help='experiment name')\n",
    "parser.add_argument('--log_freq', type=int, default=40, help='record log frequency')\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.save = 'search-{}-{}'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# seems like this is the one that creates a duplicated python file for some reason, most likely to keep a copy of whatever code is executed for easier debugging maybe?\n",
    "utils.create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))\n",
    "\n",
    "logger = MyLogger(\"my_log.log\")\n",
    "\n",
    "CLASSES = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training section\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if not torch.cuda.is_available():\n",
    "    logger.log('no gpu device available')\n",
    "    sys.exit(1)\n",
    "\n",
    "logger.log('gpu device = %d' % args.gpu)\n",
    "temp_log_string = \"args = %s\", args\n",
    "logger.log(temp_log_string)\n",
    "\n",
    "pcanet = PCANet(args.stages, args.filter_shape, args.stages_channels, args.block_size, args.block_overlap)\n",
    "train_queue, valid_queue = load_train_mnist(args)        # load dataset\n",
    "logger.log(\"load training dataset completely\")\n",
    "total_train_labels = torch.tensor([]).long()\n",
    "\n",
    "writer = SummaryWriter(args.save)       # tensorboardX\n",
    "\n",
    "# extract feature from images\n",
    "with torch.no_grad():\n",
    "    # first of all, generate eigenvector, and then execute convolution\n",
    "    stage_save_path = args.save\n",
    "    save_filename = utils.create_pickle_file_name(stage_save_path, 0)\n",
    "    for global_step, (train_images, train_labels) in enumerate(train_queue):\n",
    "        train_images = train_images.cuda()\n",
    "        total_train_labels = torch.cat((total_train_labels, train_labels))\n",
    "        utils.save_feature([train_images, train_labels], save_filename)         # this seems to save the images\n",
    "        pcanet.unrolled_stage(train_images, 0)\n",
    "\n",
    "        if global_step % args.log_freq== 0:\n",
    "            logger.log(\"init training global_step: %d\" % global_step)\n",
    "            # convert a batch of tensor into CHW format\n",
    "            grid_images = make_grid(train_images, nrow=16, padding=5, pad_value=125)\n",
    "            writer.add_image(\"raw_images_in_step_%d\" % global_step, grid_images)\n",
    "\n",
    "    total_features = torch.tensor([])       # empty tensor\n",
    "    for stage in range(args.stages):\n",
    "        logger.log('PCANet stage: %d' % stage)\n",
    "\n",
    "        # transform eigenvector to convolution kernel\n",
    "        kernel = pcanet.eigenvector_to_kernel(stage)\n",
    "\n",
    "        load_filename = utils.create_pickle_file_name(stage_save_path, stage)\n",
    "        if stage + 1 < args.stages:\n",
    "            save_filename = utils.create_pickle_file_name(stage_save_path, stage + 1)\n",
    "\n",
    "        load_filename_pointer = 0         # clear file object pointer\n",
    "        for step in range(global_step + 1):\n",
    "            train_images, train_labels, load_filename_pointer = utils.load_feature(load_filename, load_filename_pointer)\n",
    "            batch_features = pcanet.pca_conv(train_images, kernel)\n",
    "            if step % args.log_freq == 0:\n",
    "                # view the i-th image's feature map in a single batch\n",
    "                single_image_feature = utils.exchange_channel(batch_features[5])\n",
    "                grid_images = make_grid(single_image_feature, nrow=8, padding=5, pad_value=125)\n",
    "                writer.add_image(\"feature_image_in_step_%d_in_stage_%d\" % (step, stage), grid_images)\n",
    "\n",
    "            if stage + 1 < args.stages:\n",
    "                utils.save_feature([batch_features, train_labels], save_filename)\n",
    "                pcanet.unrolled_stage(batch_features, stage + 1)\n",
    "            else:\n",
    "                decimal_features = pcanet.binary_mapping(batch_features, stage)\n",
    "                final_features = pcanet.generate_histogram(decimal_features)\n",
    "                final_features = final_features.cpu()\n",
    "                total_features = torch.cat((total_features, final_features), dim=0)\n",
    "\n",
    "            if step % args.log_freq == 0:\n",
    "                logger.log(\"circulate training step: %d\" % step)\n",
    "\n",
    "        grid_kernels = make_grid(pcanet.kernel[stage], nrow=args.stages_channels[stage], padding=5, pad_value=125)\n",
    "        writer.add_image(\"kernel_in_stage_%d\" % stage, grid_kernels)\n",
    "\n",
    "    writer.close()\n",
    "    logger.log('extract feature completely, start training classifier')\n",
    "\n",
    "    # train classifier\n",
    "    classifier = LinearSVC()\n",
    "    # classifier = SVC()\n",
    "    # total_features = total_features.cpu()\n",
    "    classifier.fit(total_features, total_train_labels)\n",
    "    logger.log('classifier trained completely')\n",
    "\n",
    "    # save model\n",
    "    utils.save_model(pcanet, stage_save_path + \"/pcanet.pkl\")\n",
    "    utils.save_model(classifier, stage_save_path + \"/classifier.pkl\")\n",
    "\n",
    "    train_score = classifier.score(total_features, total_train_labels)\n",
    "    logger.log(\"score of training is %s\" % train_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation section"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : search-EXP-20230814-153131\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import torch\n",
    "import utils\n",
    "import logging\n",
    "import argparse\n",
    "from dataset_mnist import load_test_mnist\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils import MyLogger\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "parser = argparse.ArgumentParser(\"PCANet\")\n",
    "# this one fixes the issue in ipython apparently\n",
    "# https://stackoverflow.com/questions/42249982/systemexit-2-error-when-calling-parse-args-within-ipython\n",
    "parser.add_argument('-f')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--dataset_name', type=str, default='mnist', help='mnist or cifar10')\n",
    "parser.add_argument('--dataset_path', type=str, default='/dataset/', help='location of the data corpus')\n",
    "parser.add_argument('--batch_size', type=int, default=256, help='batch size')\n",
    "parser.add_argument('--stages', type=int, default=2, help='the number of stages')\n",
    "# looks like this one has to be changed to target the correct trained whatever\n",
    "parser.add_argument('--pretrained_path', type=str, default='search-EXP-20230814-135920', help='pretrained_path')\n",
    "parser.add_argument('--log_freq', type=int, default=30, help='record log frequency')\n",
    "parser.add_argument('--save', type=str, default='EXP', help='experiment name')\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.save = 'search-{}-{}'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# seems like this is the one that creates a duplicated python file for some reason, most likely to keep a copy of whatever code is executed for easier debugging maybe?\n",
    "utils.create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))\n",
    "\n",
    "logger = MyLogger(\"my_log_eval.log\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu device = 0\n",
      "('args = %s', Namespace(f='C:\\\\Users\\\\PC\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-cc11951f-281c-47ba-8812-123026cb34da.json', gpu=0, dataset_name='mnist', dataset_path='/dataset/', batch_size=256, stages=2, pretrained_path='search-EXP-20230814-135920', log_freq=30, save='search-EXP-20230814-153131'))\n",
      "load PCANet and SVM completely\n",
      "load testing dataset completely\n",
      "global_step 0, stage 1, batch accuracy 0.726562\n",
      "global_step 30, stage 1, batch accuracy 0.769531\n",
      "total accuracy 0.750200\n",
      "test completely\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not torch.cuda.is_available():\n",
    "    logger.log('no gpu device available')\n",
    "    sys.exit(1)\n",
    "\n",
    "logger.log('gpu device = %d' % args.gpu)\n",
    "temp_log_string = \"args = %s\", args\n",
    "logger.log(temp_log_string)\n",
    "\n",
    "pcanet = utils.load_model(args.pretrained_path + \"/pcanet.pkl\")\n",
    "classifier = utils.load_model(args.pretrained_path + \"/classifier.pkl\")\n",
    "logger.log(\"load PCANet and SVM completely\")\n",
    "\n",
    "test_queue, num_test = load_test_mnist(args)       # load dataset\n",
    "logger.log(\"load testing dataset completely\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    num_of_correct_samples = 0\n",
    "    for global_step, (test_images, test_labels) in enumerate(test_queue):\n",
    "        batch_size = test_images.shape[0]\n",
    "        batch_features = test_images.cuda()\n",
    "\n",
    "        # execute convolution in different stages\n",
    "        for stage in range(args.stages):\n",
    "            batch_features = pcanet.pca_conv(batch_features, pcanet.kernel[stage])\n",
    "\n",
    "        # build binary quantization mapping and generate histogram\n",
    "        decimal_features = pcanet.binary_mapping(batch_features, stage)\n",
    "        final_features = pcanet.generate_histogram(decimal_features)\n",
    "\n",
    "        # calculate the rate of correct classification\n",
    "        final_features = final_features.cpu()\n",
    "        predict_class = classifier.predict(final_features)\n",
    "        batch_accuracy = accuracy_score(predict_class, test_labels)\n",
    "\n",
    "        if global_step % args.log_freq == 0:\n",
    "            logger.log(\"global_step %d, stage %d, batch accuracy %f\" % (global_step, stage, batch_accuracy))\n",
    "\n",
    "        batch_num_of_correct_samples = utils.total_accuracy(predict_class, test_labels)\n",
    "        num_of_correct_samples += batch_num_of_correct_samples\n",
    "\n",
    "    logger.log(\"total accuracy %f\" % (num_of_correct_samples / num_test))\n",
    "    logger.log(\"test completely\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}