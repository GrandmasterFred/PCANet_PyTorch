{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This will train the network I guess\n",
    "And then I will likely need another file to test the network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : search-EXP-20230815-103200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import torch\n",
    "import utils\n",
    "import logging\n",
    "import argparse\n",
    "from pcanet import PCANet\n",
    "from dataset_mnist import load_train_mnist\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from utils import MyLogger\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "parser = argparse.ArgumentParser(\"PCANet\")\n",
    "# this one fixes the issue in ipython apparently\n",
    "# https://stackoverflow.com/questions/42249982/systemexit-2-error-when-calling-parse-args-within-ipython\n",
    "parser.add_argument('-f')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--dataset_name', type=str, default='mnist', help='mnist or cifar10')\n",
    "parser.add_argument('--dataset_path', type=str, default='/dataset/', help='location of the data corpus')\n",
    "parser.add_argument('--batch_size', type=int, default=128, help='batch size')\n",
    "# this train portion can be modified to make it faster i guess, default is 1.0\n",
    "parser.add_argument('--train_portion', type=float, default=0.01, help='portion of training data')\n",
    "parser.add_argument('--stages', type=int, default=2, help='the number of stages')\n",
    "parser.add_argument('--filter_shape', type=list, default=[7, 7], help='patch size')\n",
    "parser.add_argument('--stages_channels', type=list, default=[8, 8], help='channels in different stages')\n",
    "parser.add_argument('--block_size', type=int, default=7, help='the size of blocks')\n",
    "parser.add_argument('--block_overlap', type=float, default=0.5, help='the rate of overlap between blocks')\n",
    "parser.add_argument('--save', type=str, default='EXP', help='experiment name')\n",
    "parser.add_argument('--log_freq', type=int, default=40, help='record log frequency')\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.save = 'search-{}-{}'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# seems like this is the one that creates a duplicated python file for some reason, most likely to keep a copy of whatever code is executed for easier debugging maybe?\n",
    "utils.create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))\n",
    "\n",
    "logger = MyLogger(\"my_log.log\")\n",
    "\n",
    "CLASSES = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training section\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Output images are of batch, channel, width, height"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# the custom dataset for the headlight\n",
    "class HeadlightCustomDataset(Dataset):\n",
    "    '''\n",
    "    this will be for the custom headlight dataset generated.\n",
    "    It must return the image, as well as the class label\n",
    "\n",
    "    for the class label, it must find the corresponding file name in\n",
    "    the original source file, and them match it based on the subfolder\n",
    "    i guess\n",
    "    '''\n",
    "    def __init__(self, annotations_dir, img_dir, annotation_label_dir, transform=None, target_transform=None, split=None):\n",
    "        import os\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.annotation_label_dir = annotation_label_dir\n",
    "\n",
    "        # count number of annotations there are\n",
    "        if split is None:\n",
    "            # this means that there is no splitting of whatever\n",
    "            self.ann_files = []\n",
    "            for _, _, file in os.walk(annotations_dir):\n",
    "                self.ann_files.extend(file)\n",
    "        else:\n",
    "            # this one should just read the split file i guess\n",
    "            self.ann_files = split\n",
    "\n",
    "        # get the annotation names\n",
    "        self.ann_labels = []\n",
    "        with open(self.annotation_label_dir, 'r') as file:\n",
    "            for line in file:\n",
    "                self.ann_labels.append(line.strip())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ann_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returns the annotations as well as the image needed\n",
    "        import pandas as pd\n",
    "        from torchvision.io import read_image\n",
    "        import json\n",
    "\n",
    "        img_name = self.ann_files[idx].replace('.json', '') + '.jpg'\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = read_image(img_path)\n",
    "\n",
    "        label_path = os.path.join(self.annotations_dir, self.ann_files[idx])\n",
    "        with open(label_path, 'r') as file:\n",
    "            label = json.load(file)\n",
    "\n",
    "        # converting the labels from h, w, x, y to x1, y1, x2, y2\n",
    "        # planning to output in format of x1, y1, x2, y2, label\n",
    "        converted_labels = []\n",
    "        for i in label['annotations']:\n",
    "            # convert the annotation NAME to annotation NUMBER\n",
    "            for index, keyword in enumerate(self.ann_labels):\n",
    "                if keyword == i['name']:\n",
    "                    temp_label = index + 1\n",
    "                    break\n",
    "\n",
    "            # convert the coordinates\n",
    "            temp = [i['bounding_box']['x'],\n",
    "                    i['bounding_box']['y'],\n",
    "                    i['bounding_box']['x'] + i['bounding_box']['w'],\n",
    "                    i['bounding_box']['y'] + i['bounding_box']['h'],\n",
    "                    temp_label]\n",
    "            converted_labels.append(temp)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            converted_labels = self.target_transform(converted_labels)\n",
    "        return image, converted_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu device = 0\n",
      "('args = %s', Namespace(f='C:\\\\Users\\\\PC\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-8ed295ff-62d8-41fe-90fd-7c83154a6fd9.json', gpu=0, dataset_name='mnist', dataset_path='/dataset/', batch_size=128, train_portion=0.01, stages=2, filter_shape=[7, 7], stages_channels=[8, 8], block_size=7, block_overlap=0.5, save='search-EXP-20230815-103200', log_freq=40))\n",
      "load training dataset completely\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not torch.cuda.is_available():\n",
    "    logger.log('no gpu device available')\n",
    "    sys.exit(1)\n",
    "\n",
    "logger.log('gpu device = %d' % args.gpu)\n",
    "temp_log_string = \"args = %s\", args\n",
    "logger.log(temp_log_string)\n",
    "\n",
    "pcanet = PCANet(args.stages, args.filter_shape, args.stages_channels, args.block_size, args.block_overlap)\n",
    "train_queue, valid_queue = load_train_mnist(args)        # load dataset\n",
    "logger.log(\"load training dataset completely\")\n",
    "total_train_labels = torch.tensor([]).long()\n",
    "\n",
    "writer = SummaryWriter(args.save)       # tensorboardX\n",
    "\n",
    "counter = 0\n",
    "for global_step, (train_images, train_labels) in enumerate(train_queue):\n",
    "    print('test')\n",
    "    counter += 1\n",
    "    if counter == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}